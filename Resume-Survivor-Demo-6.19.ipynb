{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import docx2txt\n",
    "import constants as cs\n",
    "from datetime import datetime\n",
    "from dateutil import relativedelta\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFSyntaxError\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import csv\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree=None\n",
    "companies=None\n",
    "designation=None\n",
    "number=None\n",
    "designation=None\n",
    "college=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    '''\n",
    "    Helper function to extract the plain text from .pdf files\n",
    "    :param pdf_path: path to PDF file to be extracted (remote or local)\n",
    "    :return: iterator of string of extracted text\n",
    "    '''\n",
    "    # https://www.blog.pythonlibrary.org/2018/05/03/exporting-data-from-pdfs-with-python/\n",
    "    if not isinstance(pdf_path, io.BytesIO):\n",
    "        # extract text from local pdf file\n",
    "        with open(pdf_path, 'rb') as fh:\n",
    "            try:\n",
    "                for page in PDFPage.get_pages(\n",
    "                        fh,\n",
    "                        caching=True,\n",
    "                        check_extractable=True\n",
    "                ):\n",
    "                    resource_manager = PDFResourceManager()\n",
    "                    fake_file_handle = io.StringIO()\n",
    "                    converter = TextConverter(\n",
    "                        resource_manager,\n",
    "                        fake_file_handle,\n",
    "                        codec='utf-8',\n",
    "                        laparams=LAParams()\n",
    "                    )\n",
    "                    page_interpreter = PDFPageInterpreter(\n",
    "                        resource_manager,\n",
    "                        converter\n",
    "                    )\n",
    "                    page_interpreter.process_page(page)\n",
    "\n",
    "                    text = fake_file_handle.getvalue()\n",
    "                    yield text\n",
    "\n",
    "                    # close open handles\n",
    "                    converter.close()\n",
    "                    fake_file_handle.close()\n",
    "            except PDFSyntaxError:\n",
    "                return\n",
    "    else:\n",
    "        # extract text from remote pdf file\n",
    "        try:\n",
    "            for page in PDFPage.get_pages(\n",
    "                    pdf_path,\n",
    "                    caching=True,\n",
    "                    check_extractable=True\n",
    "            ):\n",
    "                resource_manager = PDFResourceManager()\n",
    "                fake_file_handle = io.StringIO()\n",
    "                converter = TextConverter(\n",
    "                    resource_manager,\n",
    "                    fake_file_handle,\n",
    "                    codec='utf-8',\n",
    "                    laparams=LAParams()\n",
    "                )\n",
    "                page_interpreter = PDFPageInterpreter(\n",
    "                    resource_manager,\n",
    "                    converter\n",
    "                )\n",
    "                page_interpreter.process_page(page)\n",
    "\n",
    "                text = fake_file_handle.getvalue()\n",
    "                yield text\n",
    "\n",
    "                # close open handles\n",
    "                converter.close()\n",
    "                fake_file_handle.close()\n",
    "        except PDFSyntaxError:\n",
    "            return\n",
    "\n",
    "\n",
    "def get_number_of_pages(file_name):\n",
    "    try:\n",
    "        if isinstance(file_name, io.BytesIO):\n",
    "            # for remote pdf file\n",
    "            count = 0\n",
    "            for page in PDFPage.get_pages(\n",
    "                        file_name,\n",
    "                        caching=True,\n",
    "                        check_extractable=True\n",
    "            ):\n",
    "                count += 1\n",
    "            return count\n",
    "        else:\n",
    "            # for local pdf file\n",
    "            if file_name.endswith('.pdf'):\n",
    "                count = 0\n",
    "                with open(file_name, 'rb') as fh:\n",
    "                    for page in PDFPage.get_pages(\n",
    "                            fh,\n",
    "                            caching=True,\n",
    "                            check_extractable=True\n",
    "                    ):\n",
    "                        count += 1\n",
    "                return count\n",
    "            else:\n",
    "                return None\n",
    "    except PDFSyntaxError:\n",
    "        return None\n",
    "\n",
    "def extract_text(file_path): \n",
    "    text = ''\n",
    "    for page in extract_text_from_pdf(file_path):\n",
    "            text += ' ' + page\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/home/amogh/Forkaia/Resume-Survivor/Amogh-Sondur-Resume.pdf\"\n",
    "\n",
    "text=extract_text(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_email(text):\n",
    "    '''\n",
    "    Helper function to extract email id from text\n",
    "    :param text: plain text extracted from resume file\n",
    "    '''\n",
    "    email = re.findall(r\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\", text)\n",
    "    if email:\n",
    "        try:\n",
    "            return email[0].split()[0].strip(';')\n",
    "        except IndexError:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mobile_number(text):\n",
    "        mob_num_regex = r'''(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)[-\\.\\s]*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})'''\n",
    "        phone = re.findall(re.compile(mob_num_regex), text)\n",
    "        if phone:\n",
    "            number = ''.join(phone[0])\n",
    "        return number\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "\n",
    "def extract_full_name(nlp_doc):\n",
    "     pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "     matcher.add('FULL_NAME', None, pattern)\n",
    "     matches = matcher(nlp_doc)\n",
    "     for match_id, start, end in matches:\n",
    "         span = nlp_doc[start:end]\n",
    "         return span.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher=Matcher(nlp.vocab)\n",
    "doc=nlp(text)\n",
    "noun_chunks=doc.noun_chunks\n",
    "\n",
    "def extract_skills(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    \n",
    "    # reading the csv file\n",
    "    data = pd.read_csv(\"/home/amogh/Forkaia/Resume-Survivor/skills.csv\") \n",
    "    \n",
    "    # extract values\n",
    "    skills = list(data.columns.values)\n",
    "     \n",
    "    skillset = []\n",
    "    \n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        if token.lower() in skills:\n",
    "            skillset.append(token)\n",
    "    \n",
    "    # check for bi-grams and tri-grams (example: machine learning)\n",
    "    for token in doc.noun_chunks:\n",
    "        token = token.text.lower().strip()\n",
    "        if token in skills:\n",
    "            skillset.append(token)\n",
    "    \n",
    "    return [i.capitalize() for i in set([i.lower() for i in skillset])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grad all general stop words\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# Education Degrees\n",
    "EDUCATION = [\n",
    "            'BE','B.E.', 'B.E', 'BS', 'B.S', \n",
    "            'ME', 'M.E', 'M.E.', 'MS', 'M.S', \n",
    "            'BTECH', 'B.TECH', 'M.TECH', 'MTECH', \n",
    "            'SSC', 'HSC', 'CBSE', 'ICSE', 'X', 'XII','Masters','Bachelors','PhD'\n",
    "        ]\n",
    "\n",
    "def extract_education(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # Sentence Tokenizer\n",
    "    nlp_text = [sent.string.strip() for sent in nlp_text.sents]\n",
    "\n",
    "    edu = {}\n",
    "    # Extract education degree\n",
    "    for index, text in enumerate(nlp_text):\n",
    "        for tex in text.split():\n",
    "            # Replace all special symbols\n",
    "            tex = re.sub(r'[?|$|.|!|]', r'', tex)\n",
    "            if tex.upper() in EDUCATION and tex not in STOPWORDS:\n",
    "                edu[tex] = text + nlp_text[index + 1]\n",
    "\n",
    "    # Extract year\n",
    "    education = []\n",
    "    for key in edu.keys():\n",
    "        year = re.search(re.compile(r'(((20|19)(\\d{2})))'), edu[key])\n",
    "        if year:\n",
    "            education.append((key, ''.join(year[0])))\n",
    "        else:\n",
    "            education.append(key)\n",
    "    return education\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_company(resume_text):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # word tokenization\n",
    "    word_tokens = nltk.word_tokenize(resume_text)\n",
    "\n",
    "    # remove stop words and lemmatize\n",
    "    filtered_sentence = [\n",
    "            w for w in word_tokens if w not\n",
    "            in stop_words and wordnet_lemmatizer.lemmatize(w)\n",
    "            not in stop_words\n",
    "        ]\n",
    "    sent = nltk.pos_tag(filtered_sentence)\n",
    "\n",
    "    # parse regex\n",
    "    cp = nltk.RegexpParser('P: {<NNP>+}')\n",
    "    cs = cp.parse(sent)\n",
    "\n",
    "    # for i in cs.subtrees(filter=lambda x: x.label() == 'P'):\n",
    "    #     print(i)\n",
    "\n",
    "    test = []\n",
    "\n",
    "    for vp in list(\n",
    "        cs.subtrees(filter=lambda x: x.label() == 'P')\n",
    "    ):\n",
    "        test.append(\" \".join([\n",
    "            i[0] for i in vp.leaves()\n",
    "            if len(vp.leaves()) >= 2])\n",
    "        )\n",
    "\n",
    "    # Search the word 'experience' in the chunk and\n",
    "    # then print out the text after it\n",
    "    x = [\n",
    "        x[x.lower().index('experience') + 10:]\n",
    "        for i, x in enumerate(test)\n",
    "        if x and 'experience' in x.lower()\n",
    "    ]\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def city_finder(text_data):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text_data)\n",
    "    for ents in doc.ents:\n",
    "        if(ents.label_ == 'GPE'):\n",
    "            return (ents.text)\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESUME_SECTIONS_GRAD = [\n",
    "                    'work experience',\n",
    "                    'accomplishments',\n",
    "                    'WORK EXPERIENCE',\n",
    "                    'experience',\n",
    "                    'education',\n",
    "                    'interests',\n",
    "                    'projects',\n",
    "                    'professional experience',\n",
    "                    'publications',\n",
    "                    'skills',\n",
    "                    'certifications',\n",
    "                    'objective',\n",
    "                    'career objective',\n",
    "                    'summary',\n",
    "                    'leadership'\n",
    "                    \n",
    "                ]\n",
    "\n",
    "\n",
    "def extract_entity_sections_grad(text):\n",
    "    '''\n",
    "    Helper function to extract all the raw text from sections of\n",
    "    resume specifically for graduates and undergraduates\n",
    "    :param text: Raw text of resume\n",
    "    :return: dictionary of entities\n",
    "    '''\n",
    "    text_split = [i.strip() for i in text.split('\\n')]\n",
    "    # sections_in_resume = [i for i in text_split if i.lower() in sections]\n",
    "    entities = {}\n",
    "    key = False\n",
    "    for phrase in text_split:\n",
    "        if len(phrase) == 1:\n",
    "            p_key = phrase\n",
    "        else:\n",
    "            p_key = set(phrase.lower().split()) & set(RESUME_SECTIONS_GRAD)\n",
    "        try:\n",
    "            p_key = list(p_key)[0]\n",
    "        except IndexError:\n",
    "            pass\n",
    "        if p_key in RESUME_SECTIONS_GRAD:\n",
    "            entities[p_key] = []\n",
    "            key = p_key\n",
    "        elif key and phrase.strip():\n",
    "            entities[key].append(phrase)\n",
    "\n",
    "    # entity_key = False\n",
    "    # for entity in entities.keys():\n",
    "    #     sub_entities = {}\n",
    "    #     for entry in entities[entity]:\n",
    "    #         if u'\\u2022' not in entry:\n",
    "    #             sub_entities[entry] = []\n",
    "    #             entity_key = entry\n",
    "    #         elif entity_key:\n",
    "    #             sub_entities[entity_key].append(entry)\n",
    "    #     entities[entity] = sub_entities\n",
    "\n",
    "    # pprint.pprint(entities)\n",
    "\n",
    "    # make entities that are not found None\n",
    "    # for entity in cs.RESUME_SECTIONS:\n",
    "    #     if entity not in entities.keys():\n",
    "    #         entities[entity] = None\n",
    "    return entities\n",
    "\n",
    "entites=extract_entity_sections_grad(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Custom Entities\"\"\"\n",
    "\n",
    "def extract_entities_wih_custom_model(custom_nlp_text):\n",
    "    '''\n",
    "    Helper function to extract different entities with custom\n",
    "    trained model using SpaCy's NER\n",
    "    :param custom_nlp_text: object of `spacy.tokens.doc.Doc`\n",
    "    :return: dictionary of entities\n",
    "    '''\n",
    "    entities = {}\n",
    "    for ent in custom_nlp_text.ents:\n",
    "        if ent.label_ not in entities.keys():\n",
    "            entities[ent.label_] = [ent.text]\n",
    "        else:\n",
    "            entities[ent.label_].append(ent.text)\n",
    "    for key in entities.keys():\n",
    "        entities[key] = list(set(entities[key]))\n",
    "    return entities\n",
    "\n",
    "\n",
    "custom_nlp=spacy.load(os.path.dirname(os.path.abspath(path)))\n",
    "custom_nlp=custom_nlp(text)\n",
    "cust_ent=extract_entities_wih_custom_model(custom_nlp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    name = cust_ent['Name'][0]\n",
    "except (IndexError, KeyError):\n",
    "    name=extract_full_name(nlp(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Amogh Sondur'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asondur@hawk.iit.edu'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email=extract_email(text)\n",
    "email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(317) 748-9445'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number=extract_mobile_number(text)\n",
    "number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CHICAGO'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city=city_finder(text)\n",
    "city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Linux',\n",
       " 'Tensorflow',\n",
       " '.net',\n",
       " 'Windows',\n",
       " 'Hadoop',\n",
       " 'Automation',\n",
       " 'Opencv',\n",
       " 'C',\n",
       " 'Aws',\n",
       " 'Microsoft excel',\n",
       " 'Java',\n",
       " 'R',\n",
       " 'Python',\n",
       " 'Excel',\n",
       " 'Javascript',\n",
       " 'Keras',\n",
       " 'Algorithms',\n",
       " 'Numpy',\n",
       " 'Machine learning',\n",
       " 'System',\n",
       " 'Pandas',\n",
       " 'Css',\n",
       " 'Engineering',\n",
       " 'Green',\n",
       " 'Mining',\n",
       " 'Spark',\n",
       " 'Matplotlib',\n",
       " 'Powerpoint',\n",
       " 'Computer science',\n",
       " 'Database',\n",
       " 'Sql',\n",
       " 'Nltk',\n",
       " 'Try',\n",
       " 'Android',\n",
       " 'Queries',\n",
       " 'Tableau',\n",
       " 'Word',\n",
       " 'Php',\n",
       " 'Hive',\n",
       " 'Analytics',\n",
       " 'Electrical',\n",
       " 'C#',\n",
       " 'Writing',\n",
       " 'Customer service',\n",
       " 'Html',\n",
       " 'Unix',\n",
       " 'Hotels',\n",
       " 'C++',\n",
       " 'Nosql',\n",
       " 'Visual']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills=extract_skills(text)\n",
    "skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Master of Science – Data Science',\n",
       " 'Bachelor’s – Computer Science and Engineering']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    degree=cust_ent['Degree']\n",
    "except KeyError:\n",
    "    pass\n",
    "degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    college=entites['College Name']\n",
    "except KeyError:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    education=entites['education']\n",
    "except KeyError:\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Master of Science – Data Science',\n",
       " 'Bachelor’s – Computer Science and Engineering']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    degree=cust_ent['Degree']\n",
    "except KeyError:\n",
    "    pass\n",
    "degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Amazon']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    companies=cust_ent['Companies worked at']\n",
    "except KeyError:\n",
    "    pass\n",
    "companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "record={\n",
    "         'Name':name,\n",
    "         'Email':email,\n",
    "         'Phone-no':number,\n",
    "         'City':city,\n",
    "         'Degree':degree,\n",
    "         'Designation':designation,\n",
    "         'Companies Worked At':companies,\n",
    "          'Skills':skills\n",
    "         }   \n",
    "\n",
    "\n",
    "import json    \n",
    "\n",
    "with open('resume.json', 'w') as outfile:\n",
    "    json.dump(record, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Name': 'Amogh Sondur',\n",
       " 'Email': 'asondur@hawk.iit.edu',\n",
       " 'Phone-no': '(317) 748-9445',\n",
       " 'City': 'CHICAGO',\n",
       " 'Degree': ['Master of Science – Data Science',\n",
       "  'Bachelor’s – Computer Science and Engineering'],\n",
       " 'Designation': None,\n",
       " 'Companies Worked At': ['Amazon'],\n",
       " 'Skills': ['Linux',\n",
       "  'Tensorflow',\n",
       "  '.net',\n",
       "  'Windows',\n",
       "  'Hadoop',\n",
       "  'Automation',\n",
       "  'Opencv',\n",
       "  'C',\n",
       "  'Aws',\n",
       "  'Microsoft excel',\n",
       "  'Java',\n",
       "  'R',\n",
       "  'Python',\n",
       "  'Excel',\n",
       "  'Javascript',\n",
       "  'Keras',\n",
       "  'Algorithms',\n",
       "  'Numpy',\n",
       "  'Machine learning',\n",
       "  'System',\n",
       "  'Pandas',\n",
       "  'Css',\n",
       "  'Engineering',\n",
       "  'Green',\n",
       "  'Mining',\n",
       "  'Spark',\n",
       "  'Matplotlib',\n",
       "  'Powerpoint',\n",
       "  'Computer science',\n",
       "  'Database',\n",
       "  'Sql',\n",
       "  'Nltk',\n",
       "  'Try',\n",
       "  'Android',\n",
       "  'Queries',\n",
       "  'Tableau',\n",
       "  'Word',\n",
       "  'Php',\n",
       "  'Hive',\n",
       "  'Analytics',\n",
       "  'Electrical',\n",
       "  'C#',\n",
       "  'Writing',\n",
       "  'Customer service',\n",
       "  'Html',\n",
       "  'Unix',\n",
       "  'Hotels',\n",
       "  'C++',\n",
       "  'Nosql',\n",
       "  'Visual']}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
